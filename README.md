Generative AI can give rise to the mass production of health-related disinformation , exposing the audience to potentially misleading healthcare policy updates and dangerous health advice. Unlike traditional disinformation campaign, generative AI models can enable the automation and the rapid dissemination of the intentional creation of disinformation tailored to target users on a large scale. Despite available disinformation detection strategies, the highly realistic and personalized AI-generated content that mimics the style of a credible source undermines their effectiveness . This empirical research seeks to improve disinformation detection tools to deal with the challenge of generative AI. I use a two-stage methodology. Initially, I conduct a comparative analysis between human-written health-related news (both real and fake) with news generated from a transformer based on the most advanced public large language model (GPT-4 launched on Mar 2023 ). The emphasis will be on linguistic and emotional features, particularly on the sentimental and textual attributes of AI-generated texts. In the subsequent stage, I train a fake news detection algorithm to outperform existing tools by incorporating features identified in the previous stage. The study will provide insights to fortify existing detection frameworks and help detect AI-generated disinformation, contributing to a more robust defence against its mass production.
